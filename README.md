# ã€Œ1ã€120åˆ†é’Ÿæ‰“é€ ä½ çš„ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“

# å…ˆå†³æ¡ä»¶
æˆ‘ä»¬é»˜è®¤å¤§å®¶å‡ï¼š
1. ç”µè„‘ä¸Šå®‰è£…æœ‰Pythonï¼ˆæˆ–è€…condaã€miniforgeç­‰ï¼‰
2. çŸ¥é“å¦‚ä½•æ‰“å¼€å‘½ä»¤è¡Œç•Œé¢ï¼ˆç»ˆç«¯ï¼‰
3. äº†è§£åŸºæœ¬çš„Pythonè¯­æ³•
4. äº†è§£æŸç§Pythonå¼€å‘IDEçš„ä½¿ç”¨ï¼ˆå¦‚PyCharmã€Visual Studioã€Cursorç­‰ï¼‰

# ç¯å¢ƒé…ç½®ï¼ˆåœ¨ç»ˆç«¯ä¸­ï¼‰
## ä¾èµ–åŒ…ç®¡ç†å™¨å®‰è£…
æˆ‘ä»¬ä½¿ç”¨ç›®å‰æœ€è¢«å¹¿æ³›ä½¿ç”¨ç°ä»£åŒ…ç®¡ç†æ–¹æ³•`uv`æ¥è¿›è¡ŒPythonç¯å¢ƒä¸Pythonåº“çš„ç®¡ç†ï¼Œå®ƒèƒ½å¤Ÿéš”ç¦»å„ä¸ªé¡¹ç›®ä¹‹é—´çš„Pythonç¯å¢ƒé¿å…å†²çªï¼Œå¹¶ä¸”èƒ½å¤Ÿä¼˜åŒ–ä¾èµ–åº“ä¹‹é—´çš„è¿ç»­å¤šçº§ä¾èµ–äº§ç”Ÿçš„å†—ä½™ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦å®‰è£…uvï¼ˆæ ¹æ®ä½ çš„Pythonå®‰è£…æ–¹å¼ä¸‰é€‰ä¸€ï¼‰ï¼š
```
pip install uv
conda install uv
mamba install uv
```

## åˆå§‹åŒ–ä¸€ä¸ªæ ‡å‡†çš„ Python é¡¹ç›®
æ–°å»ºä¸€ä¸ªé¡¹ç›®æ–‡ä»¶å¤¹`my_agent`ï¼Œå¹¶å‰å¾€æœ¬æ–‡ä»¶å¤¹ç›®å½•ä¸‹ï¼š
```
mkdir my_agent
cd my_agent
```
ä¸ºè¯¥é¡¹ç›®é…ç½®åˆå§‹çš„Pythonç¯å¢ƒï¼š
```
uv init
uv venv
source .venv/bin/activate
```
## å®‰è£…å¿…è¦ä¾èµ–
```
uv add google-adk arxiv json os litellm
```
## æ–°å»º.envæ–‡ä»¶å¹¶å†™å…¥å¦‚ä¸‹ä¿¡æ¯é…ç½®API Keyã€åœ¨ä½ çš„å¼€å‘IDEä¸­ã€‘
```
DEEPSEEK_API_KEY=ã€ç”¨ä½ çœŸå®çš„API Keyæ›¿æ¢è¿™é‡Œã€‘
```
## Agent ä»£ç å®ç°ã€åœ¨ä½ çš„å¼€å‘IDEä¸­ã€‘
æ–°å»º`agent.py`æ–‡ä»¶å¹¶å†™å…¥ï¼š
```
from google.adk.agents import Agent

import arxiv
import json
import os
from typing import List
from google.adk.models.lite_llm import LiteLlm


def search_papers(topic: str, max_results: int = 5) -> List[str]:
    """
    Search for papers on arXiv based on a topic and store their information.
    
    Args:
        topic: The topic to search for
        max_results: Maximum number of results to retrieve (default: 5)
        
    Returns:
        List of paper IDs found in the search
    """
    
    # Use arxiv to find the papers 
    client = arxiv.Client()

    # Search for the most relevant articles matching the queried topic
    search = arxiv.Search(
        query = topic,
        max_results = max_results,
        sort_by = arxiv.SortCriterion.Relevance
    )

    papers = client.results(search)
    
    # Create directory for this topic
    path = os.path.join(PAPER_DIR, topic.lower().replace(" ", "_"))
    os.makedirs(path, exist_ok=True)
    
    file_path = os.path.join(path, "papers_info.json")

    # Try to load existing papers info
    try:
        with open(file_path, "r") as json_file:
            papers_info = json.load(json_file)
    except (FileNotFoundError, json.JSONDecodeError):
        papers_info = {}

    # Process each paper and add to papers_info  
    paper_ids = []
    for paper in papers:
        paper_ids.append(paper.get_short_id())
        paper_info = {
            'title': paper.title,
            'authors': [author.name for author in paper.authors],
            'summary': paper.summary,
            'pdf_url': paper.pdf_url,
            'published': str(paper.published.date())
        }
        papers_info[paper.get_short_id()] = paper_info
    
    # Save updated papers_info to json file
    with open(file_path, "w") as json_file:
        json.dump(papers_info, json_file, indent=2)
    
    print(f"Results are saved in: {file_path}")
    
    return paper_ids

def extract_info(paper_id: str) -> str:
    """
    Search for information about a specific paper across all topic directories.
    
    Args:
        paper_id: The ID of the paper to look for
        
    Returns:
        JSON string with paper information if found, error message if not found
    """
 
    for item in os.listdir(PAPER_DIR):
        item_path = os.path.join(PAPER_DIR, item)
        if os.path.isdir(item_path):
            file_path = os.path.join(item_path, "papers_info.json")
            if os.path.isfile(file_path):
                try:
                    with open(file_path, "r") as json_file:
                        papers_info = json.load(json_file)
                        if paper_id in papers_info:
                            return json.dumps(papers_info[paper_id], indent=2)
                except (FileNotFoundError, json.JSONDecodeError) as e:
                    print(f"Error reading {file_path}: {str(e)}")
                    continue
    
    return f"There's no saved information related to paper {paper_id}."

use_model = "deepseek"

if use_model == "deepseek":
    model = LiteLlm(model="deepseek/deepseek-chat")
if use_model == "gpt-4o":
    model = LiteLlm(model="azure/gpt-4o")


root_agent = Agent(
    name="search_papers_agent",
    model=model,
    description=(
        "Agent to answer questions about the papers."
    ),
    instruction=(
        "You are a helpful agent who can answer user questions about the papers."
    ),
    tools=[search_papers, extract_info],
)
```
# è¿è¡Œ
åœ¨ç»ˆç«¯ä¸­ï¼ˆè¯¥æ–‡ä»¶å¤¹ä¸‹ï¼‰è¿è¡Œï¼š
```
uv run adk web
```
æ³¨ï¼š

1. Google ADKå…±æœ‰ä¸‰ç§å¯åŠ¨Agentçš„æ–¹å¼ï¼Œ`adk web`ã€`adk run`ã€`adk api_server`ï¼Œä»–ä»¬åˆ†åˆ«æŒ‡ï¼š
ä½¿ç”¨Google ADKå°è£…å¥½çš„Agentç•Œé¢ã€ŒåŒ…å«äº†æ•°æ®ä¼ è¾“å’ŒåŸºæœ¬çš„ä¼šè¯ç®¡ç†ï¼ˆå¦‚ä¿è¯å¤šè½®å¯¹è¯çš„æ­£å¸¸è¿è¡Œï¼‰çš„é¢„å®ç°ã€ã€ç›´æ¥ä»¥è„šæœ¬çš„å½¢å¼åœ¨ç»ˆç«¯å¯åŠ¨Agentã€å°†Agentä½œä¸ºæŸç§å…¬å¼€æœåŠ¡å¼€æ”¾ä¸ºAPIï¼Œæˆ‘ä»¬è¿™é‡Œä½¿ç”¨äº†æœ€ç›´è§‚çš„`adk web`ä¸ºå¤§å®¶æ¼”ç¤ºã€‚

2. æ‰€æœ‰ä½¿ç”¨`uv`è¿›è¡Œä¾èµ–ç®¡ç†çš„é¡¹ç›®ï¼Œå¦‚æœæƒ³åœ¨ä»¥è¯¥`uv`ç¯å¢ƒä½œä¸ºç¼–è¯‘ç¯å¢ƒè¿è¡Œï¼Œå‡éœ€ä½¿ç”¨`uv run ã€ä½ å¸Œæœ›è¿è¡Œçš„æŒ‡ä»¤ã€‘`å¯åŠ¨ï¼Œè¿è¡Œçš„æŒ‡ä»¤æœ¬èº«å¹¶æ— å˜åŒ–ã€‚

# æµ‹è¯•
å½“æµ‹è¯•æ—¶å¯ä»¥çœ‹åˆ°ç±»ä¼¼çš„æ­£ç¡®è°ƒç”¨å·¥å…·å¹¶è¿”å›ï¼Œåˆ™è®¤ä¸ºä½ çš„AgentæˆåŠŸæ­å»ºğŸ‰ğŸ‰ğŸ‰

![alt](https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/24161/f4790cbbe01a4b56be0b23b047c0ef4a/5b30c5ec-740d-4170-88bb-6d78e417d4fa.png)
